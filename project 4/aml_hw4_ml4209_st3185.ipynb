{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text_train:  167529\n",
      "class balance:  [102791  64738]\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "train =pd.read_csv('reddit_200k_train.csv', \\\n",
    "                   usecols=['body','REMOVED'], encoding = \"ISO-8859-1\")\n",
    "test =pd.read_csv('reddit_200k_test.csv', \\\n",
    "                  usecols=['body','REMOVED'], encoding = \"ISO-8859-1\")\n",
    "text_trainval, y_trainval = train['body'], train['REMOVED']\n",
    "\n",
    "print(\"length of text_train: \", len(text_trainval))\n",
    "print(\"class balance: \", np.bincount(y_trainval) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 Bag of Words and simple Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create a baseline model using a bag-of-words approach and a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_train, text_val, y_train, y_val = \\\n",
    "train_test_split(text_trainval, y_trainval, \\\n",
    "                 stratify = y_trainval, random_state = 0)\n",
    "vect = CountVectorizer()\n",
    "X_train = vect.fit_transform(text_train)\n",
    "X_val = vect.transform(text_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7589150906685347"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "lr = LogisticRegressionCV(scoring=\"roc_auc\").fit(X_train, y_train)\n",
    "lr.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Try using n-grams, characters, tf-idf rescaling and possibly other ways to tune the BoW model. Be aware that you might need to adjust the (regularization of the) linear model for different feature sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf-idf rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#tf-idf\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text \\\n",
    "import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "param_grid ={\"logisticregression__C\":[10,1,0.1],\n",
    "            \"tfidfvectorizer__ngram_range\":[(1,1),(1,2),(2,4)]}\n",
    "\n",
    "tfidf_pipe = make_pipeline(TfidfVectorizer(min_df=2,stop_words='english'), \n",
    "                        Normalizer(), \\\n",
    "                           LogisticRegression(max_iter=100, \\\n",
    "                                              solver = 'sag'))\n",
    "tfidf_grid = GridSearchCV(tfidf_pipe, \\\n",
    "                          param_grid=param_grid, cv=3, scoring=\"roc_auc\")\n",
    "\n",
    "tfidf_grid.fit(text_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training roc auc score: 0.762\n",
      "best parameters: {'logisticregression__C': 1, 'tfidfvectorizer__ngram_range': (1, 2)}\n",
      "validation-set score: 0.763\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Training roc auc score: {:.3f}\".format(tfidf_grid.best_score_))\n",
    "print(\"best parameters: {}\".format(tfidf_grid.best_params_))\n",
    "print(\"validation-set score: {:.3f}\".format(tfidf_grid.score(text_val, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-grams included stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# n-grams included stopwords and min_df =4\n",
    "param_grid ={\"logisticregression__C\":[10,1,0.1],\n",
    "            \"countvectorizer__ngram_range\":[(1,1),(1,2),(2,4)]}\n",
    "ngrams_pipe = make_pipeline(CountVectorizer(stop_words='english', min_df=4), \n",
    "                            Normalizer(), \\\n",
    "                            LogisticRegression(max_iter=100, \\\n",
    "                                               solver = 'sag'))\n",
    "ngrams_grid = GridSearchCV(ngrams_pipe, \\\n",
    "                           param_grid = param_grid, cv =3, scoring = 'roc_auc')\n",
    "ngrams_grid.fit(text_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training roc auc score: 0.761\n",
      "best parameters: {'countvectorizer__ngram_range': (1, 2), 'logisticregression__C': 1}\n",
      "validation-set score: 0.762\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Training roc auc score: {:.3f}\".format(ngrams_grid.best_score_))\n",
    "print(\"best parameters: {}\".format(ngrams_grid.best_params_))\n",
    "print(\"validation-set score: {:.3f}\".\\\n",
    "      format(ngrams_grid.score(text_val, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characters n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#characters n-grams\n",
    "param_grid ={\"logisticregression__C\":[10,1,0.1],\n",
    "            \"countvectorizer__ngram_range\":[(1,1),(1,2),(2,4)]}\n",
    "char_pipe = make_pipeline(CountVectorizer(stop_words='english', \\\n",
    "                                          analyzer =\"char_wb\", min_df=4), \n",
    "                            Normalizer(), LogisticRegression\\\n",
    "                          (max_iter=100, solver = 'sag'))\n",
    "char_grid = GridSearchCV(ngrams_pipe, param_grid = param_grid, \\\n",
    "                         cv =3, scoring = 'roc_auc')\n",
    "char_grid.fit(text_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training roc auc score: 0.761\n",
      "best parameters: {'countvectorizer__ngram_range': (1, 2), 'logisticregression__C': 1}\n",
      "validation-set score: 0.762\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Training roc auc score: {:.3f}\".format(char_grid.best_score_))\n",
    "print(\"best parameters: {}\".format(char_grid.best_params_))\n",
    "print(\"validation-set score: {:.3f}\".format(char_grid.score(text_val, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Explore other features you can derive from the text, such as html, length, punctuation, capitalization or other features you deem important from exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "removed\n",
       "False    292.768487\n",
       "True     156.719626\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length\n",
    "len_train = np.vectorize(len)(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of captilized words\n",
    "cap_train = text_train.str.findall(r'[A-Z]').str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of https\n",
    "import re\n",
    "html_train =[]\n",
    "for i in text_train:\n",
    "    count = len(re.findall(r'(?=http)', i))\n",
    "    html_train.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f =pd.DataFrame(\n",
    "    {'length': len_train,\n",
    "     'cap' : cap_train,\n",
    "     'html' : html_train\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of words\n",
    "len_val = np.vectorize(len)(text_val)\n",
    "# count the number of captilized words\n",
    "cap_val = text_val.str.findall(r'[A-Z]').str.len()\n",
    "#count the number of https\n",
    "html_val =[]\n",
    "for i in text_val:\n",
    "    count = len(re.findall(r'(?=http)', i))\n",
    "    html_val.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "d ={'length': len_val, 'cap' : cap_val, 'html' : html_val}\n",
    "X_val_f =pd.DataFrame(data = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid ={\"logisticregression__C\" : [100, 10, 1, 0.1]}\n",
    "lr_f = make_pipeline(LogisticRegression(solver = 'sag'))\n",
    "f_grid = GridSearchCV(lr_f, param_grid=param_grid, cv=3, scoring=\"roc_auc\")\n",
    "f_grid.fit(X_train_f, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training roc auc score: 0.654\n",
      "best parameters: {'logisticregression__C': 100}\n",
      "validation-set score: 0.656\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Training roc auc score: {:.3f}\".format(f_grid.best_score_))\n",
    "print(\"best parameters: {}\".format(f_grid.best_params_))\n",
    "print(\"validation-set score: {:.3f}\".format(f_grid.score(X_val_f, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a pretrained word-embedding (word2vec, glove or fasttext) instead of the bag-of-words model. Does this improve classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train = pd.read_csv('reddit_200k_train.csv', \\\n",
    "                    encoding='ISO-8859-1', index_col=0)\n",
    "test = pd.read_csv('reddit_200k_test.csv', encoding='ISO-8859-1', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## X_train, y_train, X_test, y_test\n",
    "X_train = train['body'].values\n",
    "y_train = (train['REMOVED']*1).values\n",
    "X_test = test['body'].values\n",
    "y_test = (test['REMOVED']*1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize word2vec model\n",
    "from gensim import models\n",
    "w = models.KeyedVectors.load_word2vec\\\n",
    "_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect_w2v = CountVectorizer(vocabulary=w.index2word)\n",
    "vect_w2v.fit(X_train)\n",
    "doc_train = vect_w2v.inverse_transform(vect_w2v.transform(X_train))\n",
    "doc_test = vect_w2v.inverse_transform(vect_w2v.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "y_train_0 = []\n",
    "for i in range(len(doc_train)):\n",
    "    if len(doc_train[i]) > 0:\n",
    "        docs.append(doc_train[i])\n",
    "        y_train_0.append(y_train[i])\n",
    "X_train_w2v = np.vstack([np.mean(w[doc], axis=0) for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "y_test_0 = []\n",
    "for i in range(len(doc_test)):\n",
    "    if len(doc_test[i]) > 0:\n",
    "        docs.append(doc_test[i])\n",
    "        y_test_0.append(y_test[i])\n",
    "X_test_w2v = np.vstack([np.mean(w[doc], axis=0) for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict using logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "lr_w2v = cross_validate(LogisticRegression(), \\\n",
    "                        X_train_w2v, y_train_0, scoring='roc_auc', cv=3)\n",
    "score = np.mean(lr_w2v['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation roc-auc score: 0.728\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross validation roc-auc score: {:.3f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the score is lower than what the team obtained in 1.1 and 1.2, we could conclude that the model doesn't improve classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
